{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2zI6zfCpBOXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b1a50f-9f2c-4588-e71e-1050d2d295f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-07 01:26:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-04-07 01:26:39 (150 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "W-xHgDsECC6X"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8UmFvIBCTAE",
        "outputId": "e73a3b01-4cab-44f1-a1e5-56e51777f6d6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBOxCedkCawO",
        "outputId": "24af78d4-a950-424a-fdf6-ec2b26147f22"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "AZdQeez9CebP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ce513f-4e95-4139-c399-14e7afc2e63d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the input text; characters into a reference integer; contrast with a word segment encode-decoder (tiktoken, sentenpiece)\n",
        "#encoder and decoder for tokens\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encodes an input string into a list of integers\n",
        "decode = lambda l: ''.join(itos[i] for i in l) # decodes an input list into a string\n",
        "\n",
        "print(encode(\"Hello\"))\n",
        "print(decode(encode(\"Hello\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yrl_nmPBfhh",
        "outputId": "df732787-f7e0-4d47-be27-094775ed7cd9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53]\n",
            "Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap the encoded text in a tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy3_q1JSCzop",
        "outputId": "0a7de251-93ea-4d54-b650-48772006a700"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train validation split; 90/10\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "eCp9AyM-EIBs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model:"
      ],
      "metadata": {
        "id": "PZp5QE7PFBVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# illustration of the training blocks\n",
        "block_size = 8\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1: block_size + 1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t + 1] # the target value is included in the context, makes the \n",
        "    # transformer more robust by seeing varied size contexts\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0chncupEs2E",
        "outputId": "33230b11-7328-49ef-da7a-cb3491f7807a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the blocks for encoding\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will be processed in parallel?\n",
        "block_size = 8 # maximum context length for predictions\n",
        "\n",
        "def get_batch(split):\n",
        "    #generalize a small batch of data into inputs x and y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random index \n",
        "    # numbers selected from 0 to (len(data) - block_size); subtracint block_size\n",
        "    # ensures the ability to read in an entire block - starting indices\n",
        "    \n",
        "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i + block_size + 1] for i in ix])\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "8dxYW-nnFyPy"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print(\"Inputs: \")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"Targets: \")\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVRPwquZJleF",
        "outputId": "4bb3a7ed-4c39-45b4-8971-4d4d24a6b349"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each batch has 32 independent targets with the following input x\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size):  # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"when input is {context.tolist()} the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-DC8jS-JrkG",
        "outputId": "06e987f1-bd57-48ed-9a80-ce448e6c9d25"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [24] the target is: 43\n",
            "when input is [24, 43] the target is: 58\n",
            "when input is [24, 43, 58] the target is: 5\n",
            "when input is [24, 43, 58, 5] the target is: 57\n",
            "when input is [24, 43, 58, 5, 57] the target is: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
            "when input is [44] the target is: 53\n",
            "when input is [44, 53] the target is: 56\n",
            "when input is [44, 53, 56] the target is: 1\n",
            "when input is [44, 53, 56, 1] the target is: 58\n",
            "when input is [44, 53, 56, 1, 58] the target is: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
            "when input is [52] the target is: 58\n",
            "when input is [52, 58] the target is: 1\n",
            "when input is [52, 58, 1] the target is: 58\n",
            "when input is [52, 58, 1, 58] the target is: 46\n",
            "when input is [52, 58, 1, 58, 46] the target is: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
            "when input is [25] the target is: 17\n",
            "when input is [25, 17] the target is: 27\n",
            "when input is [25, 17, 27] the target is: 10\n",
            "when input is [25, 17, 27, 10] the target is: 0\n",
            "when input is [25, 17, 27, 10, 0] the target is: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram language model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "aGpvI44VJy3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfb4ad5-65c1-4a18-da48-1d73c917f30f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb19a357b10>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C); batch, time, channel tensor\n",
        "        # time is the block size (8) and channel is the vocab size (65)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # function expects (B, T)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # logits = score for the next character in the sequence \n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get predictions\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1,:] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "\n",
        "            # sample from the distribution \n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "EH6WQ3h8MM5a"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "bvmyvOoe5BpP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAbcBkaP5Ft-",
        "outputId": "c8a44798-bf61-4baa-fe2f-83d6abe6b792"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuabP1Pw5O6_",
        "outputId": "9d354c67-8018-47fb-ae98-b8a86c00d737"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "XCJ8AZSXB3RB"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "    # sample a batch of the data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "3MONcSjRCY7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1641928-6e3b-4b2c-d265-b4616ebcc9b5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl8ExnT-jd8V",
        "outputId": "24fb08f5-250a-4916-fd06-8400e3f96fca"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical trick for self-attention"
      ],
      "metadata": {
        "id": "E8hgmQI99xmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4, 8, 2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "LtJC-n1nj94J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d947aaa-7500-4090-eb76-48c33ad1c0a5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ( 1 )"
      ],
      "metadata": {
        "id": "WkGOedE0C_U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up so that T tokens are communicating; by previous context to the current time step\n",
        "# average channels that preceed the given token -- lossy, no info on distribution\n",
        "\n",
        "# want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))  # bag of words\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "QxHyazxF-HTy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHm5rqQK_dlh",
        "outputId": "26d83e37-c9c1-406b-ace8-9d6e23ff25e1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggijYnvI_iCG",
        "outputId": "24ddc550-4bd2-4b3c-cd78-0001a41f9d79"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFZ_GZE8_pcv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more efficient matrix operation\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True) # normalizes the rows --> averaging effect \n",
        "    # on multiplication for preceeding rows\n",
        "b = torch.randint(0, 10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a = ')\n",
        "print(a)\n",
        "print('--------')\n",
        "print('b = ')\n",
        "print(b)\n",
        "print('--------')\n",
        "print('c = ')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBfA3INt_zKa",
        "outputId": "49baf539-2d89-4705-cb4f-1c492f6a01f5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = \n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--------\n",
            "b = \n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--------\n",
            "c = \n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# triangle shaped after T, used to produce running averages of the preceeding rows (as per cell 1)\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n4JbdvbBHgH",
        "outputId": "6d2cac2c-deaf-4d18-d60d-f5360240ce36"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) --> batch elements managed per batch --> (T, T) @ (T, C) --> (B, T, C)\n",
        "# identical to xbow\n",
        "xbow2[0], xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "789mA0d1CYaV",
        "outputId": "d37ab4aa-b34b-4fcd-b60d-20cf46d3126e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# third version\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # creates -inf wherever tril is 0\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGx2_HSCD0DM",
        "outputId": "0448e4a3-537c-4d65-e4ca-ae817f91a3cb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbCog4W5FBF-",
        "outputId": "42d6b36e-8ec7-4c39-eec5-728f55c1ccf5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32"
      ],
      "metadata": {
        "id": "s2DW5E4eFQZH"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shaped\n",
        "\n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C); batch, time, channel tensor\n",
        "        # time is the block size (8) and channel is the vocab size (65)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # function expects (B, T)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # logits = score for the next character in the sequence \n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get predictions\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1,:] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "\n",
        "            # sample from the distribution \n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "cB1uq0H1GMR4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention"
      ],
      "metadata": {
        "id": "yUFDxaxuIG6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: Self-attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)"
      ],
      "metadata": {
        "id": "TpaEKROJGRGL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is self-attention because the keys, queries and values are all derived from the same x, same nodes \n",
        "# communicate\n",
        "\n",
        "# each node will emit a query and key; this develops what the node has an affinity for based on the training data\n",
        "# a vowel looking for a consonant in it's past\n",
        "# query - what am I looking for?\n",
        "# key - what do I contain?\n",
        "# wei is the dot product of queries and keys --> high shows affinity\n",
        "\n",
        "# a single head\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T); T, T matrix are the affinities\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # this is the decoder block that ensures\n",
        "    # that nodes from the future do not send information to nodes in the past\n",
        "    # deleting this block will enable the encoder block\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x) # v is the feature that communicates with the other nodes about the \n",
        "             # internal information\n",
        "out = wei @ v\n",
        "\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YIlCdn5IrYM",
        "outputId": "0ec8e39a-6ac3-4339-f4bc-bfcc3b0ecdf3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each batch element has a unique weight\n",
        "wei[0]  # strength of number in the columns indicates which value the vector has an affinity for"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVsLBeG1LA9j",
        "outputId": "70c012ed-758c-4f0a-e6db-badd9ade0929"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x is private information for the token; what is shared in the network are the values v and weights based\n",
        "# on the affinity of values within the vectors \n",
        "\n",
        "# specific to self-attention\n",
        "# hyperparameters\n",
        "# batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "# block_size = 32 # what is the maximum context length for predictions?\n",
        "# max_iters = 5000\n",
        "# eval_interval = 100\n",
        "# learning_rate = 1e-3\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# eval_iters = 200\n",
        "# n_embed = 64\n",
        "# n_head = 4\n",
        "# n_layer = 4\n",
        "# dropout = 0.0"
      ],
      "metadata": {
        "id": "M4dLHN1gLCcW"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "hx2UykISVhXt"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (B,T,C)\n",
        "        q = self.query(x) # (B, T, C)\n",
        "\n",
        "        # compute attendion scores (affinities between nodes)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,C) @ (B,C,T) --> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) --> (B,T,C)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "i52Jiv_rSeVM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList((Head(head_size) for _ in range(num_heads)))\n",
        "        self.proj = nn.Linear(n_embed, n_embed) # reincorporating the skip/residual layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "rpP0Z77yCDQ3"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"linear layer followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed), # residual layer projected back into original path\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "SoO-KuhiDQjU"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed: embedding dimension, n_head: the number of heads\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed) # layer norm with dimensions \n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "# residual/skip connections are the additional addition with x\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "X2nH8mEPT1wf"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similar to pytorch batchnorm1d; this version is a layer norm\n",
        "\n",
        "class LayerNorm:\n",
        "    \n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        #parameters trained with backpropagation\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass\n",
        "        xmean = x.mean(1, keepdims=True) # batch mean; changed dim to 1 for columns\n",
        "        xvar = x.var(1, keepdims=True) # batch variance; changed dim to 1 for columns\n",
        "        xhat = (x - xmean)/torch.sqrt(xvar + self.eps) # normalize unit variance\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "    \n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "459s4A9F0RpI"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://arxiv.org/pdf/1512.03385.pdf for skip/residual connections\n",
        "# greatest contribution at optimization\n",
        "\n",
        "# dropout references: https://dl.acm.org/doi/pdf/10.5555/2627435.2670313 "
      ],
      "metadata": {
        "id": "QfnhBGzyaraI"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "#        self.sa_head = Head(n_embed)\n",
        "        # self.sa_heads = MultiHeadAttention(4, n_embed//4) # 4 heads of 8 dimensional self-attention\n",
        "        # self.ffwd = FeedForward(n_embed)\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     nn.LayerNorm(n_embed),\n",
        "        # )\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C); batch, time, channel tensor\n",
        "        # time is the block size (8) and channel is the vocab size (65)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "#        x = self.sa_heads(x)\n",
        "        x = self.blocks(x)\n",
        "#        x = self.ffwd(x) # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # function expects (B, T)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # logits = score for the next character in the sequence \n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# adjust the index to account for the block size\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1,:] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "\n",
        "            # sample from the distribution \n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "IsILFit833rs"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "4ifQT5Kx8wE4"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "wzpjTMzg8FlF"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXYnTpkI6MsE",
        "outputId": "1faa347e-e94c-439c-f28a-e260a3857c17"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.5469, val loss 4.5451\n",
            "step 500: train loss 2.1201, val loss 2.1695\n",
            "step 1000: train loss 1.6764, val loss 1.8398\n",
            "step 1500: train loss 1.5021, val loss 1.6969\n",
            "step 2000: train loss 1.3953, val loss 1.6065\n",
            "step 2500: train loss 1.3252, val loss 1.5577\n",
            "step 3000: train loss 1.2688, val loss 1.5290\n",
            "step 3500: train loss 1.2246, val loss 1.5032\n",
            "step 4000: train loss 1.1889, val loss 1.4920\n",
            "step 4500: train loss 1.1487, val loss 1.4791\n",
            "step 4999: train loss 1.1188, val loss 1.4769\n",
            "\n",
            "Your wonds have: I will seek then will preten;\n",
            "And Catals, like our ancient's sun,\n",
            "Or boaring with took in the hour,\n",
            "Then his doth to mean open end mour slavior with:\n",
            "Go, all her, and I'll sleep: I heard of them,\n",
            "And what would have, do't but dry clost? O neederflaude\n",
            "Is great the tell? He, cousin father, was done!\n",
            "My step and sour, or rought and more forget\n",
            "That any fellows looks reises, and his mispring, well'd\n",
            "And leave is with this common I'ld redeceive him indeed.\n",
            "\n",
            "PRINCE:\n",
            "I caught to hear him speak; I though he had hearm or blood,\n",
            "Where I cannot him be heir?\n",
            "O blesseed the ead, nurse of that flect gime,\n",
            "That in men years about the lord.\n",
            "\n",
            "Lord:\n",
            "Are thou yet rich walk'd is land?\n",
            "And thou debels me? if thou dost their sacred me wrong:\n",
            "Speak I previl, tell my bosom, she's brother,\n",
            "And when the bles very htmes and made.\n",
            "\n",
            "BRUKENBURY:\n",
            "I meant thee, adieu him go to an ears, yet the gass;\n",
            "For I makings a brides such a course of out\n",
            "Deserved along and see him nothings woman,\n",
            "Unless me nice here to use above her soldier,\n",
            "It back to harm and his more.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What walling lady, both the unight singuard wind\n",
            "Mercy and Sicilia? Nore not this first.\n",
            "Pures your bolds children and bold,\n",
            "The one way to beholder by,\n",
            "False with withdrawneds.\n",
            "\n",
            "ISABELLA:\n",
            "There's here here?\n",
            "\n",
            "ISABELLA:\n",
            "I, my lord; there clouds you heart\n",
            "Whose sames a sea charam as I do love;\n",
            "But she's not say 'lay profits;' but the wearing both: old\n",
            "Looks me cracked inconselice to hell, for razes\n",
            "Ere a very serped of trial?\n",
            "Then, here from himself here, to look.\n",
            "\n",
            "DIONES:\n",
            "Go coff all to me, holy daughter to findles!\n",
            "What here is casel, Clivilous for his lord!\n",
            "\n",
            "DORCASET:\n",
            "Amen! come to entraility, or\n",
            "I shall hear it, I am, not proud, I'll wript thou but seem.\n",
            "Come, where, consure! I pray you\n",
            "Within you, built that you wicke which shall\n",
            "Came in the promistance must our unnature of herself.\n",
            "\n",
            "Lord:\n",
            "So farewell, I wot.\n",
            "\n",
            "BAGUTHAS:\n",
            "Thou not chidest! thou hast\n",
            "When thus? Richas, I heard thee: from Velsce,\n",
            "We cannoth t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "id": "Z-iuiG767CRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c83178-0565-4521-b84a-c3d6b9982767"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lord Angelo his fresh king parth.\n",
            "\n",
            "DUCHESS:\n",
            "You will kill him, and whom it did;\n",
            "I shall see his worle harm with you our harm;\n",
            "Yet not goe into his good for ears to me.\n",
            "Sweet-swear joints at I know that hear me,\n",
            "Come ye withal be put to added\n",
            "His conclaims against your ball,\n",
            "Thedoching was a great's considerer,\n",
            "Since would conserved your wranches and feather.\n",
            "At shall this slews from which Caesar's\n",
            "blanchesom's friendly, have with him, and the\n",
            "reofling blastage:\n",
            "He company set, is a twicked by drawn'd my grants;\n",
            "set lady deliver for choicioys,\n",
            "and thele from out heavy away in head!\n",
            "\n",
            "HORTMIONE:\n",
            "What and help!\n",
            "Call his younds?\n",
            "Hath he liked thee more lieved to be proceders!\n",
            "This integratest honour metal's view?\n",
            "That often the womb of my marriage, gives man.\n",
            "Do not palward Henry half, fear and cot us;\n",
            "Since, or go in hastistraction, that and Jord's fruita's death,\n",
            "Because his swallor be bridale, fhilstly sire;\n",
            "I saw' both kingdom ere long mysire\n",
            "In plottle deep my spires.\n",
            "\n",
            "ROMEO:\n",
            "Justice, Master Flurius, Richard, flection was thou?\n",
            "I'll stir, but thy kin thy duke to France;\n",
            "Be have I let myself, thy daughter, York.\n",
            "There's s no father; herein in England my hand?\n",
            "\n",
            "ISABELLA:\n",
            "Shall I found the Volscasion!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Be you untan soft all to say not dance.\n",
            "\n",
            "LUCIO:\n",
            "Believe this mawn, and the divice, or me' love,\n",
            "Now no ruly less in the eye\n",
            "Thou art contented to'er clet\n",
            "At prepare. For the leight's son: 'tis proved\n",
            "By daughter quitely to me yourselves.\n",
            "\n",
            "LUCENTIO:\n",
            "'The based no doubt!'\n",
            "Whither, a' this purpose? is't on your honour?\n",
            "What he is son? O, vergain, I thought the may know? When\n",
            "The lirk of dear resoltion of this hand with harm,\n",
            "That blessed by Edward's queen? why, Henry\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, scarr'd, take espile, and for herl'd,\n",
            "As my tears to his friend, leave them surple shall\n",
            "Deaght us, while great and dance agains\n",
            "Death out the quarr'd himself cure and treaps?\n",
            "Twater arm I down protector.\n",
            "\n",
            "KING RICHARD IIII.' Now, Richard, cousin, you are\n",
            "Dost not below\n",
            "Agalled with me. Think you have slain, I will intercept.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Now, take not speak, by their brothers from our houses\n",
            "Trone's and fortune, under both with Thursday me. Cold Fiftce,\n",
            "And play'd Romeo, wrong brothers,\n",
            "Are I might mine honour, though therefore at pant;\n",
            "Without apastern for, her, making husbard!\n",
            "Made now 'shall, alm that's off!\n",
            "I'll tender crafts me, yought much, needly\n",
            "Than young with him! me hang with ack we me?\n",
            "\n",
            "ROMEO:\n",
            "No, let the maid a place one in he here,\n",
            "But, mindly record Clifford, well are king?\n",
            "If else my own sudding hance, but even thought unto\n",
            "Saint themselves his here\n",
            "At the summonsets of your thine.\n",
            "\n",
            "BENVOLIO:\n",
            "O, sweet me! then, he is and title\n",
            "It begonious more nothing but drawding with a bled,\n",
            "weathing creaters of my sonsmeless enfought not\n",
            "Than the world's gaze\n",
            "And bulchamber. Therefore I am this fools,\n",
            "As I see have shall'd, ha?\n",
            "While, it that he's heaven wrong'd that sort,\n",
            "And mean timely deadly mount with me,\n",
            "And do I well meet very his owl.\n",
            "\n",
            "KING EDWARD IV:\n",
            "O mounty times peace, for our poor journey,\n",
            "That apply mething no treasure with haste.\n",
            "\n",
            "FLORDESTA:\n",
            "O some something stamps tree!\n",
            "I'll such that wrong'd before my soles charge Mercutio.\n",
            "\n",
            "BUCKINGHAS:\n",
            "Rasolacing! who dother?\n",
            "\n",
            "PRINCE:\n",
            "Do not the king.\n",
            "\n",
            "FLORIZEL:\n",
            "He ells the king!\n",
            "Here's he loves me too the same of Catus go.\n",
            "\n",
            "STANLENSIO:\n",
            "A brother, you somethal;\n",
            "And good much advicts holest me to grace,\n",
            "Could tell them do before I hereaded,\n",
            "Become this duke, wrong are to them just,\n",
            "I have heard againt in his head own: he's body,\n",
            "I lead with his high him: he's a chastelhearing him,\n",
            "Jew'd humlent him and smilt like Esturn forsake\n",
            "And puose her help to heaven. What cover seen\n",
            "Pour lovels their heaven, mad me enry him strength\n",
            "I have you but left me.\n",
            "\n",
            "DIFFORD:\n",
            "I leave ne are dust mine or heart\n",
            "To live power to Richmond thee groan!\n",
            "And I will look woth me, but now, and by me;\n",
            "Go both, such as occupasion, judgmenting made,\n",
            "Which noble on vileant seal thee ill lulk ament!\n",
            "Will, help glad to reture them that till thither;\n",
            "And hath siz-ght grain back in note lading;\n",
            "And, being I see, God, bear thy hand,\n",
            "Edward where thou st to death be sagued\n",
            "To flinty. Dart thou lay will I live, as thou best\n",
            "As thou art the cenjurich'd?\n",
            "The seater's heart, to be wedged, unevow the fire,\n",
            "It was the mighty coal that bold times ongur fortunes\n",
            "Before me, come my peace I'ld in meetinge, aimpy\n",
            "As long curren common vain a bed and trumpet,\n",
            "The wonder your standing your tome?\n",
            "Could you shall your hatricted for them to look\n",
            "Our hour won\n",
            "Are might beholded much in coaun:\n",
            "No more,--in than a grief much may form,\n",
            "Before King Henry, let him for him; your back,\n",
            "That your command to be come hith deliverseth,\n",
            "Direction our babous, decling eard\n",
            "Where we further I call'd, with disignour,\n",
            "Have betred by mine ears of death,\n",
            "And foul noble preck, true, bath got-honour!\n",
            "\n",
            "FLORIZEL:\n",
            "\n",
            "HENRY VOLINGBY:\n",
            "From this virtuous; I know thou hadst down entreaties;\n",
            "Where our frame, I laid arm: thy friend queen,\n",
            "Or deadly, yet makes not how to me,\n",
            "Lastet attemples it owtor else,\n",
            "And jest in your mistressity, we'll wall quarrel:\n",
            "For this thing, that are match her.\n",
            "To marrib, and what king, bone's for obey good\n",
            "About to whom here, look the council: had been is en!\n",
            "Heaven done: but lady's son\n",
            "Fetrious minises, which, as he were seen so ut\n",
            "Rather for such rich her brothers: be one,\n",
            "I'll hear for her, if you're to-day straight\n",
            "Too her I do all all used, if I any buzzings\n",
            "As thought be for to me do leadness? Come,\n",
            "As women, Clifford the hear, hate I devish unthee,\n",
            "In wife, I note with her\n",
            "To prevent thee, would have their services; betwives be\n",
            "As in heard? prepare thou so, leans dear\n",
            "Volscia clong seign, whow note, and so more lives to daugar\n",
            "Than all,--plain'd too my polation,\n",
            "Is ballad-gods, for no offer'd you: here\n",
            "My daughter: come you and go beast so clouds with\n",
            "For graces, lead a sea,\n",
            "Where you gravel heard herself rote happinces:\n",
            "Do everse of heaven hersewixting your bounds,\n",
            "And made tow me the bitter, I bried here will\n",
            "Till you brook forced me a hurrent. All shall\n",
            "Borning that will\n",
            "Hath frown and his paintred dishabities\n",
            "For lady; to in let my sight resugal lose,\n",
            "Adown many time much more impless'd: as in't ear,\n",
            "That I see't my badken'd upon wing,\n",
            "Aring a dangerous day,\n",
            "And that myself unking'd by sound fall of love,\n",
            "Whetever your grace, seeking being this,\n",
            "And made the kin my sickle seat now\n",
            "Shall leight whe destick like our offidence,\n",
            "With which me, is her swifter'd:\n",
            "Be deep, her elevel, I will not be the\n",
            "to cramed my kinsmen root off! Call, my lord, wellow,\n",
            "Jesu ere they hour finds at me;\n",
            "And they use with his vew, and a cusheque,\n",
            "Of all all that view their service!\n",
            "Though hast to bark them sorrow unde's war?'\n",
            "Corm, good queen, and to death? beward you gone:\n",
            "Ay, I smellen, to answer much, as not be to do\n",
            "Whise yours false best very gives on Laudion pourt\n",
            "Ast Aunt said he wath honour'd the common her to Breta's\n",
            "With ward, witched, Camitolous, endurabris,\n",
            "Upon you in my sistering sun majeys;\n",
            "Whereof here unceased is night; for the\n",
            "peace is putient of raised\n",
            "To piece of it deservers.\n",
            "\n",
            "FRIAR Julia, let us us my proud;\n",
            "And see fortune awhip.\n",
            "\n",
            "FRIAR LAURENCES:\n",
            "O, when would begg'd\n",
            "And sa--Wedne, that made thou has that traim,\n",
            "And what thou put for these nothing such a\n",
            "Tonguele-face or morrow?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "No, Poricia little to speak; where your ignorans guests\n",
            "I unever rightly beggart soffice.\n",
            "\n",
            "HENRY PERCHIO:\n",
            "Well, my lord; hast you know'd it from that:\n",
            "And, my use, past soul commithed? whatefully looks it?\n",
            "\n",
            "EXTER:\n",
            "Bid me that it be a poil'd in blood\n",
            "In blows? both got about them, get his son-book!\n",
            "If I dine banish! and where was I warrant to come?\n",
            "\n",
            "DUKE OF YORK:\n",
            "Farewell stooth, I am guess both to bed.\n",
            "\n",
            "KING RICHARD II:\n",
            "Hear not, learn for be, here marry, is he.\n",
            "If I part some infound sort another!\n",
            "Shall I hope away, and say I throw heed so?\n",
            "\n",
            "DUCESTER:\n",
            "It is worthy blood: if thou stand where died xcellenting?\n",
            "\n",
            "HASTINGS:\n",
            "How doth your highness! what you would have wont our heavens!\n",
            "Live, with bounce of no world this unles last,\n",
            "In that son Edward's dear's glory false reats,\n",
            "That He which shall king, Henry, high mintaint down,\n",
            "To mine enemy hunt the soumpast dip tears\n",
            "Of the englect that die, they every ward,\n",
            "That we'll go against a while, sainst forth;\n",
            "'The gape I sentleman,' would make auge to him;\n",
            "And intemption of those grandles such\n",
            "That I herein for love, and monthy sistery,\n",
            "And ne'er mourned we had luck'd vengeably to queen,\n",
            "And to go by thee, all the sistile and rins!\n",
            "What is the sun, when he's fant made for wor, being no letick\n",
            "Upon yet us the rest bitter for the means but,\n",
            "To be sewel but a this wife's graff'd,\n",
            "So nobleman shall makes no hard and prophet?\n",
            "Go, no gorsets, or a love honour to ourselves,\n",
            "The cause that each my concient gains--\n",
            "Take off my beready vices,--thou sexiest drawn'd,\n",
            "And thou had'st the better but out o' the holy\n",
            "Which, thoughts at Which weak no mistone?\n",
            "Where is his grace of much?\n",
            "He would not he? our commands, awhip well\n",
            "Can him from the hideoping his willing lawful breaths.\n",
            "\n",
            "SICINIUS:\n",
            "Will't thou hast how thy mund?\n",
            "\n",
            "SICINIUS:\n",
            "I see in deceip,\n",
            "That appathers that the gentlemans there.\n",
            "Come to me, revire from\n",
            "thee\n",
            "accident: to use the fifth high face the war's restrence\n",
            "her aidefriend by the petect of name,\n",
            "and let hear life\n",
            "to them speak of tears.\n",
            "\n",
            "CORIOLANUS:\n",
            "I'll am too mecilour,\n",
            "I'll have does whelse pathers' meaven subject!\n",
            "My so, talk gest a blest in like common sleep,\n",
            "Three alone me, we, that seasest with detrqueristing\n",
            "Deep that is too fair that were lifet me ere\n",
            "Her here to the purettion; I'ld ever in\n",
            "The sicites, the world for the my cold join'd,\n",
            "The Lord weep deflock, who spair cheering in warn\n",
            "These mistressly, for the trim-heading thereof\n",
            "Your sons of with much both close of Margaret apple;\n",
            "And most himself arrest along over kingly.\n",
            "Within this high own soldiers more earth,\n",
            "And treachering before the Edward shall.\n",
            "\n",
            "GLOUCESTER:\n",
            "Whom will weap that our fix'd health kitest.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kQpetrpFTSk"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}