{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc96fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/PaCmpygFfXo?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&t=3777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004f39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ed892",
   "metadata": {},
   "source": [
    "## Neural Network Bigram model\n",
    "Train the network to predict probability of the second character of the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8b4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee437918",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)} # +1 to make special character at 0\n",
    "#stoi['<S>'] = 26\n",
    "#stoi['<E>'] = 27\n",
    "# single special token, place at index 0\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836f2d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams\n",
    "\n",
    "# 2 lists; inputs and the outputs (second character of bigram)\n",
    "xs, ys = [], []\n",
    "\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1] \n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# lower case assumes the dtype; upper case coerces to float32\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e55e58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e965b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ffe807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this stage the xs and ys values are the indices of the characters\n",
    "# convert to one-hot encoding in order have meaningful value for use in the weights of the nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# encode the full 27 characters as the number of classes\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # cast as float for later forward-backward pass\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6abc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fddd9bb5ae0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)\n",
    "\n",
    "# e m m a in one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f285eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea9b9a4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first neuron\n",
    "\n",
    "# inital weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator = g, requires_grad = True) # 27 character values, 27 neurons (evaluations)\n",
    "                                         # need to explicitly require gradient in the calculation\n",
    "# W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fae8ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "         -2.9643e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
       "         -4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01,\n",
       "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0730e-02,\n",
       "          2.4968e+00,  2.4448e+00],\n",
       "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
       "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "         -2.1259e+00,  9.6041e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
       "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
       "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
       "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
       "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
       "          5.4108e-01, -1.1646e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5, 27) @ (27, 1) --> (5, 1)\n",
    "# (5, 27) @ (27, 27) --> (5, 27)\n",
    "\n",
    "xenc@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36278fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0379, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@W)[3, 13] #firing rate of the 13th neuron on the 4th input; dot product of the input 3 and the 13th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e8abda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0379, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum() # actual equation for dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48408377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7940,  0.7888,  0.9730,  0.3326,  1.3309,  0.9708,  0.2129,  1.8311,\n",
       "          1.0824,  2.4710,  0.6242,  2.1964,  0.7200,  0.6486,  3.9469, 18.7908,\n",
       "          4.7673,  0.1967,  1.9683,  0.4315,  2.6775,  0.8621,  0.2277,  1.5656,\n",
       "          0.9317, 12.1434, 11.5281],\n",
       "        [ 1.6038,  4.4060,  1.3737,  2.8830, 11.0032,  1.5972,  0.5187,  1.8527,\n",
       "          0.5369,  1.6654,  3.8818,  1.2642,  0.6339,  0.9987,  0.5995,  1.7432,\n",
       "          1.6073,  0.2499,  5.0680,  1.1876,  2.6871,  1.6596,  2.7728,  0.1486,\n",
       "          0.6521,  0.1193,  2.6128],\n",
       "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
       "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
       "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
       "          4.6866,  1.8232,  0.4921],\n",
       "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
       "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
       "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
       "          4.6866,  1.8232,  0.4921],\n",
       "        [ 0.5117,  0.2953,  1.3541,  0.3422,  2.0701,  1.0524,  3.7043,  0.4483,\n",
       "          0.4272,  0.1642,  3.4984,  0.2936,  3.3753,  0.3811,  0.7929,  0.7064,\n",
       "          1.3944,  0.2655,  3.0723,  1.8156,  1.5816,  1.0555,  0.1755,  1.1225,\n",
       "          2.2327,  1.7179,  0.3120]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret the outputs of the neural net as the log counts of the layers; positive and negative values\n",
    "# in order to create probabilities take the exponitiation of the log counts\n",
    "\n",
    "(xenc@W).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1059b",
   "metadata": {},
   "source": [
    "### Foward Pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805a6c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction of the following character (column) from a given character (row)\n",
    "\n",
    "logits = xenc @ W # log counts\n",
    "\n",
    "# softmax function: https://en.wikipedia.org/wiki/Softmax_function\n",
    "counts = logits.exp() # equivalent to the N count matrix\n",
    "probs = counts / counts.sum(1, keepdims = True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f54091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape # shape for the given input (emma, with start and end token '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf245f0",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- input dataset; inputs and labels\n",
    "- each input receives 27 different weights; the likelihood of the next character being what is represented by that column; start with random values\n",
    "- values are exonentiated and divided by the sum of the rows (softmax reduction to probabilities)\n",
    "- probabilities represent probability for each character from the columns following the characters from the rows\n",
    "- all of the steps are differentiable --> backpropagation to adjust the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5150e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "bigram example: 1: .e (indexes 0, 5)\n",
      "input to the neural net:  0\n",
      "output probabilities from the neural net:  tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character):  5\n",
      "probability assigned by the net to the correct character:  0.01228625513613224\n",
      "log likelihood:  -4.399273872375488\n",
      "negative log likelihood:  4.399273872375488\n",
      "---------------\n",
      "bigram example: 2: em (indexes 5, 13)\n",
      "input to the neural net:  5\n",
      "output probabilities from the neural net:  tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character):  13\n",
      "probability assigned by the net to the correct character:  0.018050700426101685\n",
      "log likelihood:  -4.014570713043213\n",
      "negative log likelihood:  4.014570713043213\n",
      "---------------\n",
      "bigram example: 3: mm (indexes 13, 13)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character):  13\n",
      "probability assigned by the net to the correct character:  0.026691533625125885\n",
      "log likelihood:  -3.623408794403076\n",
      "negative log likelihood:  3.623408794403076\n",
      "---------------\n",
      "bigram example: 4: ma (indexes 13, 1)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character):  1\n",
      "probability assigned by the net to the correct character:  0.07367686182260513\n",
      "log likelihood:  -2.6080665588378906\n",
      "negative log likelihood:  2.6080665588378906\n",
      "---------------\n",
      "bigram example: 5: a. (indexes 1, 0)\n",
      "input to the neural net:  1\n",
      "output probabilities from the neural net:  tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character):  0\n",
      "probability assigned by the net to the correct character:  0.014977526850998402\n",
      "log likelihood:  -4.201204299926758\n",
      "negative log likelihood:  4.201204299926758\n",
      "================\n",
      "average negative log likelihood, i.e. loss =  3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "# for the first word example\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('---------------')\n",
    "    print(f\"bigram example: {i + 1}: {itos[x]}{itos[y]} (indexes {x}, {y})\")\n",
    "    print('input to the neural net: ', x)\n",
    "    print('output probabilities from the neural net: ', probs[i])\n",
    "    print('label (actual next character): ', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the correct character: ', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood: ', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood: ', nll.item())\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('================')\n",
    "print('average negative log likelihood, i.e. loss = ', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "398e78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the loss by tuning the W settings using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae10b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50082209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0181, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0267, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0737, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0150, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probabilites predicted for the indexes of emma\n",
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fee9a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilites that the neural network assigns to the correct next character\n",
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8ffeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized calculation of loss\n",
    "loss = - probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64914ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16388209",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5be9d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gradients to 0\n",
    "W.grad = None\n",
    "loss.backward()  # network remembers the operations in the network; 1 weighted layer and 1 activation layer (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974b9571",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# W.grad # influence of the given weights on the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single layer nn, no need to loop\n",
    "\n",
    "# new weights += learning rate * gradient\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87e48c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7693049907684326"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "328fe86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2532f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator = g, requires_grad = True)\n",
    "#if you attempt to make the values close to 0 there will be less variability within the probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a2d1d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4829697608947754\n",
      "2.4829440116882324\n",
      "2.4829187393188477\n",
      "2.482893943786621\n",
      "2.4828693866729736\n",
      "2.4828450679779053\n",
      "2.482820987701416\n",
      "2.482797384262085\n",
      "2.482774257659912\n",
      "2.4827513694763184\n",
      "2.4827287197113037\n",
      "2.482706308364868\n",
      "2.482684373855591\n",
      "2.4826626777648926\n",
      "2.4826412200927734\n",
      "2.4826200008392334\n",
      "2.4825992584228516\n",
      "2.4825785160064697\n",
      "2.482558250427246\n",
      "2.4825379848480225\n",
      "2.482518196105957\n",
      "2.4824984073638916\n",
      "2.4824793338775635\n",
      "2.4824600219726562\n",
      "2.4824414253234863\n",
      "2.4824228286743164\n",
      "2.4824044704437256\n",
      "2.482386589050293\n",
      "2.482368230819702\n",
      "2.4823505878448486\n",
      "2.482333183288574\n",
      "2.4823157787323\n",
      "2.4822990894317627\n",
      "2.4822821617126465\n",
      "2.4822652339935303\n",
      "2.4822492599487305\n",
      "2.4822328090667725\n",
      "2.4822170734405518\n",
      "2.482201099395752\n",
      "2.4821853637695312\n",
      "2.4821698665618896\n",
      "2.482154607772827\n",
      "2.4821391105651855\n",
      "2.4821243286132812\n",
      "2.482109785079956\n",
      "2.482095241546631\n",
      "2.4820809364318848\n",
      "2.4820663928985596\n",
      "2.4820523262023926\n",
      "2.482038736343384\n",
      "2.482024908065796\n",
      "2.482011556625366\n",
      "2.4819979667663574\n",
      "2.4819846153259277\n",
      "2.481971502304077\n",
      "2.4819586277008057\n",
      "2.481945753097534\n",
      "2.481933116912842\n",
      "2.4819202423095703\n",
      "2.481908082962036\n",
      "2.481895685195923\n",
      "2.4818837642669678\n",
      "2.481872081756592\n",
      "2.4818601608276367\n",
      "2.4818482398986816\n",
      "2.4818365573883057\n",
      "2.481825113296509\n",
      "2.481813907623291\n",
      "2.481802463531494\n",
      "2.4817917346954346\n",
      "2.481780529022217\n",
      "2.481769561767578\n",
      "2.4817588329315186\n",
      "2.481748580932617\n",
      "2.4817378520965576\n",
      "2.481727361679077\n",
      "2.4817168712615967\n",
      "2.4817070960998535\n",
      "2.481696844100952\n",
      "2.48168683052063\n",
      "2.4816765785217285\n",
      "2.4816672801971436\n",
      "2.4816575050354004\n",
      "2.4816477298736572\n",
      "2.4816384315490723\n",
      "2.4816293716430664\n",
      "2.4816195964813232\n",
      "2.4816105365753174\n",
      "2.4816012382507324\n",
      "2.4815921783447266\n",
      "2.481583833694458\n",
      "2.481574773788452\n",
      "2.4815661907196045\n",
      "2.481557607650757\n",
      "2.48154878616333\n",
      "2.4815404415130615\n",
      "2.481532335281372\n",
      "2.4815237522125244\n",
      "2.481515407562256\n",
      "2.4815075397491455\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes = 27).float() # input to the network\n",
    "    logits = xenc @ W # predict log-counts <-- this is the step that will be elaborated on with later models \n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims = True) # probabilities for next character\n",
    "#     loss = -probs[torch.arange(num), ys].log().mean()\n",
    "\n",
    "    # loss with regularization: the second sum term drives the values towards 0, reducing variability and \n",
    "    #    minimizing the impact of the changes\n",
    "    # the first sum term drives the values to approach the predicted value -- minimize loss\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() \n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set zero gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update \n",
    "    W.data += -50 * W.grad  # arrives at the same loss as the counting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa20d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaymoryles.\n",
      "kondlaisah.\n",
      "anchthizarie.\n"
     ]
    }
   ],
   "source": [
    "# sampling from a neural network to generate probable words\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    out = []\n",
    "    # start with the start token\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # proceed through the rows driven by the probability of the preceeding character\n",
    "        # using the probabilities from a single tensor array will produce the same results\n",
    "        # from the probabilistic model\n",
    "        # p = P[ix]\n",
    "    \n",
    "    \n",
    "        # with the neural network\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes = 27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims = True) # probabilities for next character\n",
    "    \n",
    "    \n",
    "        # select a new row based on the proability distribution of second characters of the current row\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix]) \n",
    "        if ix == 0: # if the selected next character is the end token break\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db853195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact same results, same underlying model produced with probabilities and neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f0801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a98b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab0536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
