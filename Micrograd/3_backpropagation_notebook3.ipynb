{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5858a5",
   "metadata": {},
   "source": [
    "## Building out the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4b77a",
   "metadata": {},
   "source": [
    "From [Andrej Karpathy: The spelled-out intro to neural networks and backpropagation: building micrograd ](https://youtu.be/VMj-3S1tku0?t=5978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89da35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ee1dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fdf38",
   "metadata": {},
   "source": [
    "# PyTorch Demonstration\n",
    "duplicates the features of Micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea12cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4b46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch has the goal of efficiency by producing operations in parallel by working on \n",
    "# tensors instead of scaler values \n",
    "\n",
    "# tensors are N dimensional arrays of scalers\n",
    "t = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "print(t.shape)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e8a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default pytorch as float32 precision\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26763339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch requires tensors instead of scalers, here tensors are 1D arrays\n",
    "# by default pytorch has dtype float32, cast as double for dtype float64 to mirror python\n",
    "# pytorch assumes leaf nodes do not require gradients, requires_grad = True calculates gradients on leaves\n",
    "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
    "\n",
    "b = torch.Tensor([6.8813735870195432]).double(); x1.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328e7a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe8878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x1*w1 + x2*w2 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5dc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.tanh(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c7e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n"
     ]
    }
   ],
   "source": [
    "print(o.data.item())\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153bb062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "print('----')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bffee7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(o.item())\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571489a4",
   "metadata": {},
   "source": [
    "# In Micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca58985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph -- from a Value Object\n",
    "    \n",
    "    nodes, edges = set(), set()\n",
    "    \n",
    "    def build(v):\n",
    "    # build reaches up out of scope to access nodes and edges... \n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                # recursive call to elaborate all of the child nodes \n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a39febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    \n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        \n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        # each node displays label, data, and gradient at that leaf\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape = 'record')\n",
    "        \n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            \n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "            \n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b730633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    # _children and _op at init requires values, used for history\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        # gradient at each leaf/Value\n",
    "        self.grad = 0 # initially set to 0 with the assumption that the leaf/Value\n",
    "                      # does not effect the overall outcome (Loss Function)\n",
    "            \n",
    "        # backward as a attribute of the Value object\n",
    "        # stores the gradient calculation at a given value (node)\n",
    "        # by default None, depends on the operation and the children nodes\n",
    "        self._backward = lambda: None\n",
    "            \n",
    "        # to capture the previous values and operations\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # operation performed at the node\n",
    "        self.label = label # name of the node\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # necessary to print out something readable -- otherwise shows\n",
    "        # the function and location in memory\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        # other must be a second Value object\n",
    "        # pass children values to capture history\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        # closure to manage the backpropagation local to the operation\n",
    "        def _backward():\n",
    "            # addition propagates the gradient forward \n",
    "#            self.grad = 1.0 * out.grad\n",
    "#            other.grad = 1.0 * out.grad\n",
    "            # += is required if a Value object is used more than once, otherwise\n",
    "            # the operation will over-write itself\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        # an addition node has this function as the action for backpropagation\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        \n",
    "        def _backward():\n",
    "#            self.grad = out.grad * other.data\n",
    "#            other.grad = out.grad * self.data\n",
    "            # += is required if a Value object is used more than once, otherwise\n",
    "            # the operation will over-write itself\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward    \n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports ints and floats\"\n",
    "        out = Value(self.data ** other, (self,), f\"**{other}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): # use case other * self\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):  # for self/other\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __neg__(self): # - self\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) *out.grad # local gradient chained to the prior gradient\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a1e264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        # takes a number of inputs on initialization\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # w * x + b <-- forward pass of neuron in the network\n",
    "        \n",
    "        # create the activation of the neuron\n",
    "        # python sum takes a second argument, where to start the sum -- self.b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "\n",
    "        # return the layer of the neuron passed through an regularization function\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2784baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is input data, the weights and biases will be trained from the network... \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d47c5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.9863700181144841)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized values\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce91270",
   "metadata": {},
   "source": [
    "### Layer of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28d176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of neurons evaluated independently\n",
    "\n",
    "class Layer:\n",
    "    # defined by number of inputs and outputs \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    # less concise code with the same return\n",
    "#         params = []\n",
    "#         for neuron in self.neurons:\n",
    "#             ps = neuron.parameters()\n",
    "#             params.extend(ps)\n",
    "#         return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8685402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.9970934107637096),\n",
       " Value(data=0.9895868071156434),\n",
       " Value(data=0.9989143021311471)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Layer(2,3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ea62b",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "multiple layers of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f05e9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    #nouts defines the size of layers for MLP\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # x is \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "771af427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8049700608291973)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass of MLP\n",
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4,4,1]) # 3 inputs passing through 2 layers of 4 and a single output unit\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3265667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "522af7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example; binary classifer nn; given 4 samples, derive 4 values \n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75212111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8049700608291973),\n",
       " Value(data=0.37417956209333747),\n",
       " Value(data=-0.1499491610068778),\n",
       " Value(data=0.755680931853579)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial, untrained outputs \n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832af8c",
   "metadata": {},
   "source": [
    "Tune the nn by observing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9749debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean square error loss\n",
    "loss = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40ad5af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.038036677172967),\n",
       " Value(data=1.8883694688750368),\n",
       " Value(data=0.7225864288729109),\n",
       " Value(data=0.059691807059935514)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8c84d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.7086843819808504)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimize this number...\n",
    "loss = sum(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb2ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dcdbeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.335181345066107"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "843a9a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16150411701751666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dc0bebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# diagram of the data, weights and gradients within the nn\n",
    "# xs is immutable - this is the data given for the problem\n",
    "# the w values are the ones to be adjusted through the gradients\n",
    "# paramters are the methods on the neurons, layers and MLP that return weights and biases\n",
    "# of the neural net\n",
    "\n",
    "# draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa99cbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.16150411701751666),\n",
       " Value(data=0.5237885046510045),\n",
       " Value(data=0.645650248367226),\n",
       " Value(data=-0.04955044632824479),\n",
       " Value(data=-0.20051054243668975),\n",
       " Value(data=-0.3981010019958944),\n",
       " Value(data=-0.708260022944112),\n",
       " Value(data=0.7560073937041072),\n",
       " Value(data=-0.8264845568076951),\n",
       " Value(data=-0.34467673281587663),\n",
       " Value(data=-0.8403862327005653),\n",
       " Value(data=0.020382741149430172),\n",
       " Value(data=0.6823684981447007),\n",
       " Value(data=0.9508410941237921),\n",
       " Value(data=-0.4729534863257183),\n",
       " Value(data=-0.5552236811559363),\n",
       " Value(data=-0.4207480300090467),\n",
       " Value(data=0.43505322237413613),\n",
       " Value(data=0.6912233912910715),\n",
       " Value(data=0.8528368428619546),\n",
       " Value(data=-0.6795429472684902),\n",
       " Value(data=-0.33008442519508985),\n",
       " Value(data=-0.9772171672259984),\n",
       " Value(data=-0.6344815688829812),\n",
       " Value(data=-0.79310116341313),\n",
       " Value(data=0.09088596448039787),\n",
       " Value(data=0.7556857420389691),\n",
       " Value(data=-0.39684057698508357),\n",
       " Value(data=0.6974886617520921),\n",
       " Value(data=-0.6221810143716133),\n",
       " Value(data=0.2910762453528042),\n",
       " Value(data=0.7213043630958418),\n",
       " Value(data=-0.1526824486763163),\n",
       " Value(data=0.10345177249350379),\n",
       " Value(data=0.6245218262039338),\n",
       " Value(data=-0.15209502635373973),\n",
       " Value(data=-0.6497686187901239),\n",
       " Value(data=-0.9038337373231551),\n",
       " Value(data=-0.8968693227668654),\n",
       " Value(data=0.7274637611361388),\n",
       " Value(data=-0.39202067676828656)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n is the MLP\n",
    "n.parameters() # 41 weights and biases in the nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be38d35",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b50be4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.7086843819808504)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2819514",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20366df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b29c7d",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eafae8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the gradients express a vector that increases the loss --> use a negative value\n",
    "\n",
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d224ba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1148004901161945"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "205c4c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.7086843819808504)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "809c324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, -1.0, -1.0, 1.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4f7d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8049700608291973),\n",
       " Value(data=0.37417956209333747),\n",
       " Value(data=-0.1499491610068778),\n",
       " Value(data=0.755680931853579)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dae9c1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.1148004901161945),\n",
       " Value(data=0.5419171759480438),\n",
       " Value(data=0.6355287424947764),\n",
       " Value(data=-0.06388225721582987),\n",
       " Value(data=-0.3315647651121827),\n",
       " Value(data=-0.370912363645728),\n",
       " Value(data=-0.754732440723496),\n",
       " Value(data=0.6944800526126003),\n",
       " Value(data=-0.828025240524645),\n",
       " Value(data=-0.3459553119665867),\n",
       " Value(data=-0.8373622773342956),\n",
       " Value(data=0.01929708151026685),\n",
       " Value(data=0.48315574069538225),\n",
       " Value(data=0.9716414796468695),\n",
       " Value(data=-0.5427051573536429),\n",
       " Value(data=-0.65430015269308),\n",
       " Value(data=-0.41643723736247085),\n",
       " Value(data=0.43412365626243293),\n",
       " Value(data=0.6794938555644163),\n",
       " Value(data=0.8535082147101573),\n",
       " Value(data=-0.6693029916459798),\n",
       " Value(data=-0.30494300253135476),\n",
       " Value(data=-0.9788046432986597),\n",
       " Value(data=-0.6916466990313559),\n",
       " Value(data=-0.7791062708846048),\n",
       " Value(data=0.14926062718721234),\n",
       " Value(data=0.7845111432973997),\n",
       " Value(data=-0.40305831893250305),\n",
       " Value(data=0.6416839796937411),\n",
       " Value(data=-0.6093439998230881),\n",
       " Value(data=0.3479108108874645),\n",
       " Value(data=0.6994571927800763),\n",
       " Value(data=-0.15009537011623844),\n",
       " Value(data=0.15174637543076558),\n",
       " Value(data=0.6133059312267535),\n",
       " Value(data=-0.2002308678123531),\n",
       " Value(data=-0.5819184923717099),\n",
       " Value(data=-0.9359387040772951),\n",
       " Value(data=-0.88842240672908),\n",
       " Value(data=0.7101764890126259),\n",
       " Value(data=-0.46559119815327316)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e1cfe",
   "metadata": {},
   "source": [
    "### Automating the cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cec3cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.010634399512334164\n",
      "1 0.010381458410525119\n",
      "2 0.010140150017367431\n",
      "3 0.009909693189271238\n",
      "4 0.009689375034491271\n",
      "5 0.009478543628413988\n",
      "6 0.009276601641514882\n",
      "7 0.00908300074939758\n",
      "8 0.008897236715241636\n",
      "9 0.008718845052210872\n",
      "10 0.008547397187614356\n",
      "11 0.0083824970624293\n",
      "12 0.008223778109647086\n",
      "13 0.00807090056313556\n",
      "14 0.00792354905562292\n",
      "15 0.007781430470225714\n",
      "16 0.007644272014858601\n",
      "17 0.007511819493027149\n",
      "18 0.007383835748042889\n",
      "19 0.007260099260717371\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    \n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "    \n",
    "    # zero out the previous gradients\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "        \n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82c1a827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9657586999390567),\n",
       " Value(data=-0.9727198956728598),\n",
       " Value(data=-0.9511911693408983),\n",
       " Value(data=0.9455837653962965)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d7539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
